\documentclass{article}
\usepackage{fullpage,amsmath,amsthm,graphicx,enumitem,amssymb}
\usepackage[hidelinks]{hyperref}
\usepackage{todonotes}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows.meta, bending}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{question}[thm]{Question}
\newenvironment{solution}{\noindent\textit{Solution:}}{}

\newcommand{\reals}{\mathbb{R}}

\title{ASEN 5264 Decision Making under Uncertainty\\
       Homework 2: Markov Decision Processes}

\begin{document}

\maketitle

\section*{Conceptual Questions}

\begin{question}\label{q:q} (10 pts)
    \begin{enumerate}[label=\alph*)]
        \item Describe the difference between the reward function and the state-action ($Q$) value function.
        \item Write down an equation for the state-action value in terms of $R$, $T$, and $V$.
    \end{enumerate}
\end{question}

\begin{question}\label{q:mdp} (25 pts)
    Consider a game with 3 squares in a horizontal line drawn on paper, a token, and a die. Each turn, the player can either reset or roll the die. If the player rolls and the die shows an odd number, the token is moved one square to the right, and if an even number is rolled, the token is moved two squares to the right (in both cases stopping at the rightmost square\footnote{If the die is rolled from the middle square or right square, it will always end up in the right square.}). If the player resets, the token is always moved to the leftmost square. If the reset occurs when the token is in the middle square, two points are added; if the player resets when the token is on the right square, a point is subtracted.
    
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[
            > = Stealth,
            node distance = 4cm,
            on grid,
            state/.style = {circle, draw, minimum size=1.0cm, font=\large},
            every edge/.style = {draw, ->, >=Stealth, auto, bend left=20, thick},
            every loop/.style = {looseness=4, thick, min distance=15mm}
        ]
        
            % States
            \node[state] (L) {$L$};
            \node[state] (M) [right=of L] {$M$};
            \node[state] (R) [right=of M] {$R$};
        
            % Roll actions from L
            \path (L.180) edge[loop, in=120, out=60, looseness=5, min distance=15mm,] 
            node[align=center, above=3mm] {reset \\ $r=0$} (L.179);
            \path (L) edge[bend left=15] node[above=0mm] {roll: 0.5} (M);
            \path (L) edge[bend left=30] node[above=0mm] {roll: 0.5} (R);
        
            % Roll actions from M
            \path (M) edge[bend left=15] node[align=center, above=0mm] {reset: $r=+2$} (L);
            \path (M) edge[bend left=15] node[above=0mm] {roll: $1$} (R);
        
            % Actions from R
            \path (R) edge[bend left=30] node[align=center, below=2mm] {reset: $r = -1$} (L);
            
            % UPDATED LOOP POSITION
            \path (R.359) edge[loop, in=300, out=240, looseness=5, min distance=15mm] 
            node[align=center, right=0.1mm, below=1mm] {roll: $1$} (R.0);
        
            % Labels
            \node[below=0.65cm of L] {\small Left};
            \node[below=0.65cm of M] {\small Middle};
            \node[below=0.65cm of R] {\small Right};
        
        \end{tikzpicture}
    \end{figure}
    
    \begin{enumerate}[label=\alph*)]
        \item Formulate this problem as an MDP (write down $S$, $A$, $T$, and $R$).
        \item Assume the game lasts for exactly 3 turns. Assuming no discounting ($\gamma = 1.0$), find the optimal actions for each state for the first turn that maximizes the expected cumulative reward.
        \item Now assume an infinite horizon with a discount factor of $\gamma = 0.95$. Calculate the value of the policy found in part (b), and determine if it remains optimal in this setting. 
        % \item Calculate the optimal value when the token is in the leftmost square assuming a discount factor of $\gamma = 0.95$.\footnote{Hint: Guess an optimal policy, evaluate that optimal policy, and then verify that it satisfies the Bellman equation.}
        \item Suppose you are not sure that the die is fair (i.e. whether it will yield odd and even with equal probability). Give finite upper and lower bounds for the accumulated discounted score that you can expect to receive with discount $\gamma = 0.95$.
    \end{enumerate}
\end{question}

\section*{Exercise}

\begin{question} \label{q:gw}
    (Value iteration for Grid World, 35 pts)

    Solve the MDP \texttt{HW2.grid\_world} with your own implementation of value iteration with a discount of $\gamma=0.95$ and plot the resulting value function. All of the necessary information to solve this problem can be extracted with the \texttt{HW2.transition\_matrices} and \texttt{HW2.reward\_vectors} functions, and plotting can be accomplished with \texttt{POMDPModelTools.render(HW2.grid\_world, color=v)} where \texttt{v} is the value function. See the starter code and function docstrings for more information.
\end{question}

\section*{Challenge Problem}

\begin{question}\label{q:acas}
    (Value iteration for ACAS, 20 pts autograder, 10 pts code)

Your task is to find the optimal value function for an Aircraft Collision Avoidance System (ACAS). The encounter model will be specified as a Markov decision process, and your task will be to compute the value function for discount $\gamma=0.99$ using value iteration or another suitable algorithm that you implement. The continuous physical state space can be discretized at different levels of granularity, and the goal is to find the value function for the finest discretization possible.

A model with discretization level \texttt{n} can be constructed with
\begin{verbatim}
                        m = HW2.UnresponsiveACASMDP(n)
\end{verbatim}
The higher \texttt{n} is, the finer the discretization and the larger the state space. Again, all of the information needed to solve this problem can be extracted with the \texttt{HW2.transition\_matrices} and \texttt{HW2.reward\_vectors} functions, so you can start with your code from Question \ref{q:gw}.

The score received for solving the problem is \texttt{n}. You must submit your code for this problem along with the \texttt{results.json} file from executing \texttt{HW2.evaluate(v, "email@colorado.edu")} where \texttt{v} is the value function vector\footnote{\texttt{HW2.evaluate} will check the value function with a tolerance of $1 \times 10^{-6}$. However, the probabilities and rewards contain some small numbers, so results can vary slightly depending on the way these are added and multiplied. Therefore we recommend using a tolerance of $1 \times 10^{-8}$.}. A score of \texttt{n} = 7 or higher will receive full credit\footnote{Hints: If you run out of memory, consider using the \texttt{sparse} keyword argument mentioned in the docstring of \texttt{transition\_matrices}. By taking advantage of the structure of the problem, it is possible to attain a score of \texttt{n} = 20 with less than 10 minutes of computation time on a single core of a i7 laptop processor.}.

\end{question}


% TODO: Revise this!

\section*{Deliverables}
\subsection*{For the Homework 2 Gradescope Assignment:}
\begin{itemize}
    \item A pdf containing answers to Questions \ref{q:q} and \ref{q:mdp} and the plot of the value function for \ref{q:gw}.
\end{itemize}

\subsection*{For the Homework 2 Code Gradescope Assignment:}
\begin{itemize}
    \item Your code for Questions \ref{q:gw} and \ref{q:acas}.
    \item \texttt{results.json}
\end{itemize}

\end{document}
